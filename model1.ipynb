{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hamming_loss, accuracy_score, f1_score, classification_report\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Cargar el dataset preprocesado\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_encoded.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, classification_report\n",
    "import optuna\n",
    "\n",
    "# Cargar el dataset preprocesado\n",
    "df = pd.read_csv('dataset_encoded.csv')\n",
    "\n",
    "# Separar las variables predictoras y la variable objetivo\n",
    "X = df.drop('custcat', axis=1)\n",
    "y = df['custcat']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Función de evaluación mejorada\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "    # Entrenamiento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas detalladas\n",
    "    print(f\"\\nDetailed Report for {model_name}:\")\n",
    "    print(\"Classification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Métricas tradicionales\n",
    "    metrics = {\n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
    "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'train_hamming': hamming_loss(y_train, y_train_pred),\n",
    "        'val_hamming': hamming_loss(y_val, y_val_pred),\n",
    "        'test_hamming': hamming_loss(y_test, y_test_pred),\n",
    "        'train_f1': f1_score(y_train, y_train_pred, average='weighted'),\n",
    "        'val_f1': f1_score(y_val, y_val_pred, average='weighted'),\n",
    "        'test_f1': f1_score(y_test, y_test_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_val, y_train_val, cv=5, scoring='accuracy')\n",
    "    metrics['cv_mean'] = np.mean(cv_scores)\n",
    "    metrics['cv_std'] = np.std(cv_scores)\n",
    "    \n",
    "    # Detección de overfitting más sofisticada\n",
    "    overfitting_threshold = 0.05\n",
    "    if metrics['train_accuracy'] - metrics['val_accuracy'] > overfitting_threshold:\n",
    "        print(f\"⚠️ Posible overfitting en {model_name}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Funciones objetivo para Optuna\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 32),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "def objective_gb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 32),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**params, random_state=42)\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "def objective_nn(trial):\n",
    "    params = {\n",
    "        'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50,50), (100,50)]),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-5, 1e-1),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'adaptive'])\n",
    "    }\n",
    "    model = MLPClassifier(**params, max_iter=1000, random_state=42)\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "def objective_knn(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 20),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
    "    }\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "\n",
    "# Función para optimizar modelos con Optuna\n",
    "def optimize_model(objective, n_trials=100, model_name=\"\"):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Best value: {study.best_value:.4f}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# Optimizar cada modelo\n",
    "print(\"Optimizing Random Forest...\")\n",
    "best_params_rf = optimize_model(objective_rf, n_trials=100, model_name=\"Random Forest\")\n",
    "\n",
    "print(\"\\nOptimizing Gradient Boosting...\")\n",
    "best_params_gb = optimize_model(objective_gb, n_trials=100, model_name=\"Gradient Boosting\")\n",
    "\n",
    "print(\"\\nOptimizing Neural Network...\")\n",
    "best_params_nn = optimize_model(objective_nn, n_trials=100, model_name=\"Neural Network\")\n",
    "\n",
    "print(\"\\nOptimizing KNN...\")\n",
    "best_params_knn = optimize_model(objective_knn, n_trials=100, model_name=\"KNN\")\n",
    "\n",
    "# Crear modelos con los mejores parámetros\n",
    "best_models = {\n",
    "    'Random Forest': RandomForestClassifier(**best_params_rf, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(**best_params_gb, random_state=42),\n",
    "    'Neural Network': MLPClassifier(**best_params_nn, max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(**best_params_knn)\n",
    "}\n",
    "\n",
    "# Evaluar los modelos optimizados\n",
    "results = {}\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating optimized {name}...\")\n",
    "    results[name] = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, name)\n",
    "\n",
    "# Imprimir resultados resumidos\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nResumen de {name} optimizado:\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
